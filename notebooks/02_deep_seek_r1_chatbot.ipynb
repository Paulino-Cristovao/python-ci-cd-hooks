{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfc6cd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# âœ… Device Selection (MPS for Mac, CUDA for NVIDIA, fallback to CPU)\n",
    "device: torch.device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('data/sample_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daad4914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product is amazing! I love it.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible experience, would not recommend.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decent quality, but could be better.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent value for money.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not worth the price.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Very satisfied with the purchase.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review sentiment\n",
       "0        This product is amazing! I love it.  positive\n",
       "1  Terrible experience, would not recommend.  negative\n",
       "2       Decent quality, but could be better.   neutral\n",
       "3                 Excellent value for money.  positive\n",
       "4                       Not worth the price.  negative\n",
       "5          Very satisfied with the purchase.  positive"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "898e8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4205ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment from review\n",
    "def get_sentiment(review):\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    sentiment = outputs.logits.argmax().item()\n",
    "    if sentiment == 0:\n",
    "        return \"negative\"\n",
    "    elif sentiment == 1:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test set\n",
    "test_data['predicted_sentiment'] = test_data['review'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.25\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.08      0.33      0.13         4\n",
      "weighted avg       0.06      0.25      0.10         4\n",
      "\n",
      "Review: This product is amazing! I love it.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Review: Serviceable, but not outstanding.\n",
      "Actual Sentiment: neutral\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Review: Neither good nor bad, just okay.\n",
      "Actual Sentiment: neutral\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Review: Terrible experience, would not recommend.\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linoospaulinos/miniforge3/envs/py-learn-py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/linoospaulinos/miniforge3/envs/py-learn-py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/linoospaulinos/miniforge3/envs/py-learn-py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_data['sentiment'], test_data['predicted_sentiment'])\n",
    "report = classification_report(test_data['sentiment'], test_data['predicted_sentiment'])\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "print(f'Classification Report:\\n{report}')\n",
    "\n",
    "# Print sentences with their actual and predicted sentiments\n",
    "for index, row in test_data.iterrows():\n",
    "    print(f\"Review: {row['review']}\\nActual Sentiment: {row['sentiment']}\\nPredicted Sentiment: {row['predicted_sentiment']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-learn-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
